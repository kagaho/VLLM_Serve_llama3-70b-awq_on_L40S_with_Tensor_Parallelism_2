{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22f1098",
   "metadata": {},
   "source": [
    "- Throughput (tokens/sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664ca122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: /mnt/elita/soundwave/models/llama3-70b-awq\n",
      "n=50  input_len=1000  output_len=50\n",
      "latency  p50 1.486s  p95 1.488s  p99 1.489s  mean 1.486s\n",
      "tok/s    p50 33.6   p95 33.7   p99 33.7   mean 33.6\n"
     ]
    }
   ],
   "source": [
    "import time, statistics, requests\n",
    "\n",
    "BASE_URL = \"http://localhost:8002\"\n",
    "N = 50\n",
    "INPUT_LEN = 1000\n",
    "OUTPUT_LEN = 50\n",
    "\n",
    "model_id = requests.get(f\"{BASE_URL}/v1/models\", timeout=10).json()[\"data\"][0][\"id\"]\n",
    "print(\"Using model:\", model_id)\n",
    "\n",
    "prompt = \"hello \" * INPUT_LEN\n",
    "\n",
    "tok_per_s = []\n",
    "lat = []\n",
    "\n",
    "for _ in range(N):\n",
    "    t0 = time.perf_counter()\n",
    "    r = requests.post(\n",
    "        f\"{BASE_URL}/v1/completions\",\n",
    "        json={\n",
    "            \"model\": model_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": OUTPUT_LEN,\n",
    "            \"temperature\": 0,\n",
    "            \"stream\": False,\n",
    "        },\n",
    "        timeout=300,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    data = r.json()\n",
    "    # OpenAI-style usage fields (vLLM usually provides these)\n",
    "    out_tokens = data.get(\"usage\", {}).get(\"completion_tokens\", OUTPUT_LEN)\n",
    "\n",
    "    dt = t1 - t0\n",
    "    lat.append(dt)\n",
    "    tok_per_s.append(out_tokens / dt)\n",
    "\n",
    "tok_s = sorted(tok_per_s)\n",
    "lat_s = sorted(lat)\n",
    "\n",
    "def pct(arr, p):\n",
    "    return arr[int(round(p * (len(arr)-1)))]\n",
    "\n",
    "print(f\"n={N}  input_len={INPUT_LEN}  output_len={OUTPUT_LEN}\")\n",
    "print(f\"latency  p50 {pct(lat_s,0.50):.3f}s  p95 {pct(lat_s,0.95):.3f}s  p99 {pct(lat_s,0.99):.3f}s  mean {statistics.mean(lat):.3f}s\")\n",
    "print(f\"tok/s    p50 {pct(tok_s,0.50):.1f}   p95 {pct(tok_s,0.95):.1f}   p99 {pct(tok_s,0.99):.1f}   mean {statistics.mean(tok_per_s):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3af43",
   "metadata": {},
   "source": [
    "test throughput under concurrency\n",
    "\n",
    "Run this (10 concurrent workers, total 100 requests). It reports aggregate tokens/sec across the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7acf9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: /mnt/elita/soundwave/models/llama3-70b-awq\n",
      "req=100  conc=10  input_len=1000  output_len=50\n",
      "per-req latency p50 1.741s p95 1.763s mean 1.735s\n",
      "aggregate throughput: 286.0 output_tokens/sec  (wall=17.48s, total_out_tokens=5000)\n"
     ]
    }
   ],
   "source": [
    "import time, statistics, requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "BASE_URL = \"http://localhost:8002\"\n",
    "CONCURRENCY = 10\n",
    "TOTAL_REQ = 100\n",
    "INPUT_LEN = 1000\n",
    "OUTPUT_LEN = 50\n",
    "\n",
    "model_id = requests.get(f\"{BASE_URL}/v1/models\", timeout=10).json()[\"data\"][0][\"id\"]\n",
    "print(\"Using model:\", model_id)\n",
    "\n",
    "prompt = \"hello \" * INPUT_LEN\n",
    "\n",
    "def one_request():\n",
    "    t0 = time.perf_counter()\n",
    "    r = requests.post(\n",
    "        f\"{BASE_URL}/v1/completions\",\n",
    "        json={\"model\": model_id, \"prompt\": prompt, \"max_tokens\": OUTPUT_LEN, \"temperature\": 0},\n",
    "        timeout=300,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    dt = time.perf_counter() - t0\n",
    "    data = r.json()\n",
    "    out_tokens = data.get(\"usage\", {}).get(\"completion_tokens\", OUTPUT_LEN)\n",
    "    return dt, out_tokens\n",
    "\n",
    "t_start = time.perf_counter()\n",
    "dts = []\n",
    "tokens = 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=CONCURRENCY) as ex:\n",
    "    futs = [ex.submit(one_request) for _ in range(TOTAL_REQ)]\n",
    "    for f in as_completed(futs):\n",
    "        dt, out = f.result()\n",
    "        dts.append(dt)\n",
    "        tokens += out\n",
    "\n",
    "wall = time.perf_counter() - t_start\n",
    "\n",
    "dts_s = sorted(dts)\n",
    "def pct(arr, p):\n",
    "    return arr[int(round(p * (len(arr)-1)))]\n",
    "\n",
    "print(f\"req={TOTAL_REQ}  conc={CONCURRENCY}  input_len={INPUT_LEN}  output_len={OUTPUT_LEN}\")\n",
    "print(f\"per-req latency p50 {pct(dts_s,0.50):.3f}s p95 {pct(dts_s,0.95):.3f}s mean {statistics.mean(dts):.3f}s\")\n",
    "print(f\"aggregate throughput: {tokens / wall:.1f} output_tokens/sec  (wall={wall:.2f}s, total_out_tokens={tokens})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
