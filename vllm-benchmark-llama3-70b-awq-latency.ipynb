{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb08fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, statistics, requests, json\n",
    "\n",
    "BASE_URL = \"http://localhost:8002\"\n",
    "N = 50\n",
    "INPUT_LEN = 100\n",
    "OUTPUT_LEN = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12de075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: /mnt/elita/soundwave/models/llama3-70b-awq\n"
     ]
    }
   ],
   "source": [
    "# 1) discover model id\n",
    "models = requests.get(f\"{BASE_URL}/v1/models\", timeout=10).json()\n",
    "model_id = models[\"data\"][0][\"id\"]\n",
    "print(\"Using model:\", model_id)\n",
    "\n",
    "prompt = \"hello \" * INPUT_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6625f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = []\n",
    "for i in range(N):\n",
    "    t0 = time.perf_counter()\n",
    "    r = requests.post(\n",
    "        f\"{BASE_URL}/v1/completions\",\n",
    "        json={\n",
    "            \"model\": model_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": OUTPUT_LEN,\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "        timeout=300,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    _ = r.json()\n",
    "    latencies.append(time.perf_counter() - t0)\n",
    "\n",
    "lat_sorted = sorted(latencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f847e713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=50\n",
      "p50  0.177s\n",
      "p95  0.179s\n",
      "p99  0.240s\n",
      "mean 0.179s\n"
     ]
    }
   ],
   "source": [
    "def pct(p):\n",
    "    return lat_sorted[int(round(p * (len(lat_sorted)-1)))]\n",
    "\n",
    "print(f\"n={N}\")\n",
    "print(f\"p50  {pct(0.50):.3f}s\")\n",
    "print(f\"p95  {pct(0.95):.3f}s\")\n",
    "print(f\"p99  {pct(0.99):.3f}s\")\n",
    "print(f\"mean {statistics.mean(latencies):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29288c78",
   "metadata": {},
   "source": [
    "What it means (for your exact test):\n",
    "\n",
    "- p50 ~ 177 ms: half your requests finished in ≤ 0.177s\n",
    "\n",
    "- p95 ~ 179 ms: almost all requests finished around the same time (very low jitter)\n",
    "\n",
    "- p99 ~ 240 ms: 1% tail spikes up to ~240 ms\n",
    "\n",
    " - mean ~ 179 ms: consistent with p50/p95 → stable run\n",
    "\n",
    "Given your settings (~100 “hello ” tokens worth of prompt string + max_tokens=50, batch=1, sequential requests), this looks like you’re measuring mostly single-request end-to-end HTTP latency + generation with little queueing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88461f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: /mnt/elita/soundwave/models/llama3-70b-awq\n",
      "n=50\n",
      "TTFT  p50 0.060s  p95 0.060s  p99 0.076s  mean 0.060s\n",
      "TOTAL p50 0.177s  p95 0.177s  p99 0.193s  mean 0.177s\n"
     ]
    }
   ],
   "source": [
    "import time, statistics, requests\n",
    "\n",
    "BASE_URL = \"http://localhost:8002\"\n",
    "N = 50\n",
    "INPUT_LEN = 100\n",
    "OUTPUT_LEN = 50\n",
    "\n",
    "# Discover model id\n",
    "model_id = requests.get(f\"{BASE_URL}/v1/models\", timeout=10).json()[\"data\"][0][\"id\"]\n",
    "print(\"Using model:\", model_id)\n",
    "\n",
    "prompt = \"hello \" * INPUT_LEN\n",
    "\n",
    "ttft = []\n",
    "total = []\n",
    "\n",
    "for i in range(N):\n",
    "    t0 = time.perf_counter()\n",
    "    with requests.post(\n",
    "        f\"{BASE_URL}/v1/completions\",\n",
    "        json={\n",
    "            \"model\": model_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": OUTPUT_LEN,\n",
    "            \"temperature\": 0,\n",
    "            \"stream\": True,\n",
    "        },\n",
    "        stream=True,\n",
    "        timeout=300,\n",
    "    ) as r:\n",
    "        r.raise_for_status()\n",
    "\n",
    "        first_token_time = None\n",
    "        # vLLM streams as SSE: lines like \"data: {...}\" and ends with \"data: [DONE]\"\n",
    "        for raw in r.iter_lines(decode_unicode=True):\n",
    "            if not raw:\n",
    "                continue\n",
    "            if raw.startswith(\"data: \"):\n",
    "                data = raw[len(\"data: \"):]\n",
    "                if data.strip() == \"[DONE]\":\n",
    "                    break\n",
    "                if first_token_time is None:\n",
    "                    first_token_time = time.perf_counter()\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "    if first_token_time is None:\n",
    "        raise RuntimeError(\"Did not receive any streamed token data; check server streaming support.\")\n",
    "    ttft.append(first_token_time - t0)\n",
    "    total.append(t1 - t0)\n",
    "\n",
    "ttft_s = sorted(ttft)\n",
    "total_s = sorted(total)\n",
    "def pct(arr, p):\n",
    "    return arr[int(round(p * (len(arr)-1)))]\n",
    "\n",
    "print(f\"n={N}\")\n",
    "print(f\"TTFT  p50 {pct(ttft_s,0.50):.3f}s  p95 {pct(ttft_s,0.95):.3f}s  p99 {pct(ttft_s,0.99):.3f}s  mean {statistics.mean(ttft):.3f}s\")\n",
    "print(f\"TOTAL p50 {pct(total_s,0.50):.3f}s  p95 {pct(total_s,0.95):.3f}s  p99 {pct(total_s,0.99):.3f}s  mean {statistics.mean(total):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c1bc3",
   "metadata": {},
   "source": [
    "What your numbers say:\n",
    "\n",
    "- TTFT p50 ≈ 60 ms (p99 76 ms)\n",
    "    → the server starts streaming the first token very fast. That’s mostly request handling + prefill start and suggests no queueing in this run.\n",
    "\n",
    "- TOTAL p50 ≈ 177 ms (p99 193 ms)\n",
    "    → end-to-end completion finishes quickly and tightly.\n",
    "\n",
    "- Decode time estimate (TOTAL − TTFT):\n",
    "    p50 ≈ 0.177 − 0.060 = 0.117 s for generating up to 50 tokens\n",
    "    → rough decode rate ≈ 50 / 0.117 ≈ 427 tokens/s (very rough because your prompt/HTTP overhead are included, but good for comparing configs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef63bb",
   "metadata": {},
   "source": [
    "### increasing the prompt to stress prefill, e.g. INPUT_LEN=1000 (keep output 50) and compare TTFT again. \n",
    "\n",
    "- Same cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd01f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: /mnt/elita/soundwave/models/llama3-70b-awq\n",
      "n=50\n",
      "TTFT  p50 0.051s  p95 0.052s  p99 0.508s  mean 0.060s\n",
      "TOTAL p50 1.486s  p95 1.487s  p99 1.953s  mean 1.495s\n"
     ]
    }
   ],
   "source": [
    "import time, statistics, requests\n",
    "\n",
    "BASE_URL = \"http://localhost:8002\"\n",
    "N = 50\n",
    "INPUT_LEN = 1000\n",
    "OUTPUT_LEN = 50\n",
    "\n",
    "# Discover model id\n",
    "model_id = requests.get(f\"{BASE_URL}/v1/models\", timeout=10).json()[\"data\"][0][\"id\"]\n",
    "print(\"Using model:\", model_id)\n",
    "\n",
    "prompt = \"hello \" * INPUT_LEN\n",
    "\n",
    "ttft = []\n",
    "total = []\n",
    "\n",
    "for i in range(N):\n",
    "    t0 = time.perf_counter()\n",
    "    with requests.post(\n",
    "        f\"{BASE_URL}/v1/completions\",\n",
    "        json={\n",
    "            \"model\": model_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": OUTPUT_LEN,\n",
    "            \"temperature\": 0,\n",
    "            \"stream\": True,\n",
    "        },\n",
    "        stream=True,\n",
    "        timeout=300,\n",
    "    ) as r:\n",
    "        r.raise_for_status()\n",
    "\n",
    "        first_token_time = None\n",
    "        # vLLM streams as SSE: lines like \"data: {...}\" and ends with \"data: [DONE]\"\n",
    "        for raw in r.iter_lines(decode_unicode=True):\n",
    "            if not raw:\n",
    "                continue\n",
    "            if raw.startswith(\"data: \"):\n",
    "                data = raw[len(\"data: \"):]\n",
    "                if data.strip() == \"[DONE]\":\n",
    "                    break\n",
    "                if first_token_time is None:\n",
    "                    first_token_time = time.perf_counter()\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "    if first_token_time is None:\n",
    "        raise RuntimeError(\"Did not receive any streamed token data; check server streaming support.\")\n",
    "    ttft.append(first_token_time - t0)\n",
    "    total.append(t1 - t0)\n",
    "\n",
    "ttft_s = sorted(ttft)\n",
    "total_s = sorted(total)\n",
    "def pct(arr, p):\n",
    "    return arr[int(round(p * (len(arr)-1)))]\n",
    "\n",
    "print(f\"n={N}\")\n",
    "print(f\"TTFT  p50 {pct(ttft_s,0.50):.3f}s  p95 {pct(ttft_s,0.95):.3f}s  p99 {pct(ttft_s,0.99):.3f}s  mean {statistics.mean(ttft):.3f}s\")\n",
    "print(f\"TOTAL p50 {pct(total_s,0.50):.3f}s  p95 {pct(total_s,0.95):.3f}s  p99 {pct(total_s,0.99):.3f}s  mean {statistics.mean(total):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e79713",
   "metadata": {},
   "source": [
    "TOTAL p50 jumped from ~0.177s → ~1.486s\n",
    "That’s expected: much more input to process.\n",
    "\n",
    "TTFT p50 is still ~51 ms, but p99 spikes to ~508 ms\n",
    "That pattern usually means: most requests are fine, but occasionally one request hits a slow path (e.g., cache miss / warmup / occasional scheduling delay / background work). Since your p95 is still ~52 ms, it’s a rare tail event.\n",
    "\n",
    "Decode time estimate (p50):\n",
    "TOTAL − TTFT ≈ 1.486 − 0.051 = 1.435 s for ~50 output tokens\n",
    "→ rough output rate ≈ 50 / 1.435 ≈ 35 tokens/s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
